# Quick Start Guide - Local Testing

## Prerequisites Check

Before starting, verify you have:
- âœ… API keys configured in `backend/.env`
- Docker Desktop running (for full stack) OR
- Python 3.11+ and Node.js 18+ (for backend/frontend only)

## Option A: Full Stack with Docker (Recommended)

### 1. Start Docker Desktop
- Open Docker Desktop application
- Wait for the whale icon to be steady (not animated)

### 2. Start Infrastructure Services
```bash
cd C:\Users\adminidiakhoa\Demo\Evo_AI
docker-compose up -d postgres redis minio
```

### 3. Initialize Database
```bash
cd backend
python -m venv venv
.\venv\Scripts\activate  # Windows
pip install -r requirements.txt
python -m evo_ai.database.init_db
```

### 4. Start Backend API
```bash
# In backend directory with venv activated
uvicorn evo_ai.api.app:app --reload --host 0.0.0.0 --port 8000
```

### 5. Start Frontend (new terminal)
```bash
cd frontend
npm install
npm run dev
```

### 6. Access the Platform
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs
- Grafana: http://localhost:3001 (username: admin, password: admin)
- Jaeger: http://localhost:16686

## Option B: Backend + Frontend Only (No Docker)

If Docker is not available, you can test with just the backend and frontend:

### 1. Set Up Backend
```bash
cd backend

# Create virtual environment
python -m venv venv
.\venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Update .env for in-memory database (optional for testing)
# DATABASE_URL=sqlite:///./test.db  # Add this to .env

# Initialize database
python -m evo_ai.database.init_db

# Start backend
uvicorn evo_ai.api.app:app --reload --port 8000
```

### 2. Set Up Frontend (new terminal)
```bash
cd frontend

# Install dependencies
npm install

# Create .env.local
echo NEXT_PUBLIC_API_URL=http://localhost:8000 > .env.local

# Start development server
npm run dev
```

### 3. Test the API
Open http://localhost:8000/docs in your browser to see the interactive API documentation.

## Quick API Tests

Once the backend is running, test with curl:

### 1. Health Check
```bash
curl http://localhost:8000/health
```

Expected response:
```json
{
  "status": "healthy",
  "timestamp": "2025-12-25T18:00:00Z"
}
```

### 2. Create a Campaign
```bash
curl -X POST "http://localhost:8000/api/campaigns" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Campaign",
    "description": "My first evolution experiment",
    "config": {
      "max_rounds": 5,
      "variants_per_round": 3,
      "evaluators": ["llm_judge"]
    }
  }'
```

### 3. List Campaigns
```bash
curl http://localhost:8000/api/campaigns
```

### 4. View API Documentation
Open http://localhost:8000/docs in your browser for interactive testing.

## Frontend Testing

1. Open http://localhost:3000
2. Navigate to "Campaigns" page
3. Click "New Campaign" button
4. Fill in the form:
   - Name: "Test Campaign"
   - Max Rounds: 5
   - Variants per Round: 3
   - Select evaluator: LLM Judge
5. Click "Create Campaign"
6. You should see your new campaign in the list

## Troubleshooting

### Backend won't start
```bash
# Check Python version
python --version  # Should be 3.11+

# Check if port 8000 is in use
netstat -ano | findstr :8000

# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

### Frontend won't start
```bash
# Clear cache and reinstall
rm -rf node_modules .next
npm install
npm run dev
```

### Database connection errors
```bash
# If using Docker, check PostgreSQL is running
docker ps | findstr postgres

# If using SQLite, just delete the file and reinitialize
rm test.db
python -m evo_ai.database.init_db
```

### API key errors
- Verify keys are in `backend/.env`
- Ensure no quotes around keys
- Restart backend after changing .env

## Next Steps

Once everything is running:

1. **Create a campaign** via the UI or API
2. **Execute a round** to see agents in action
3. **View lineage visualization** to see variant evolution
4. **Check analytics dashboard** for insights
5. **View reports** generated by ReporterAgent

## Monitoring & Debugging

### View Backend Logs
```bash
# Backend terminal shows structured logs
# Look for agent execution traces
```

### View Traces (if Jaeger is running)
- Open http://localhost:16686
- Search for service: "evo-ai-backend"
- Find traces by campaign_id or agent type

### View Metrics (if Prometheus/Grafana running)
- Open http://localhost:3001
- Login: admin/admin
- Explore dashboards for system metrics

## Sample Test Workflow

```bash
# 1. Create campaign
CAMPAIGN_ID=$(curl -s -X POST "http://localhost:8000/api/campaigns" \
  -H "Content-Type: application/json" \
  -d '{"name":"Quick Test","config":{"max_rounds":3,"variants_per_round":2,"evaluators":["llm_judge"]}}' \
  | jq -r '.id')

echo "Created campaign: $CAMPAIGN_ID"

# 2. Start campaign
curl -X POST "http://localhost:8000/api/campaigns/$CAMPAIGN_ID/start"

# 3. Execute first round (async)
curl -X POST "http://localhost:8000/api/tasks/rounds/execute?campaign_id=$CAMPAIGN_ID" \
  -H "Content-Type: application/json" \
  -d '{"round_number":1}'

# 4. Check campaign status
curl "http://localhost:8000/api/campaigns/$CAMPAIGN_ID"

# 5. View rounds
curl "http://localhost:8000/api/campaigns/$CAMPAIGN_ID/rounds"
```

## Help & Support

If you encounter issues:
- Check logs in the terminal
- Visit http://localhost:8000/docs for API reference
- Review [DEPLOYMENT.md](docs/DEPLOYMENT.md) for detailed setup
- Check [ARCHITECTURE.md](docs/ARCHITECTURE.md) for system design

Happy experimenting! ðŸ§¬ðŸ¤–
